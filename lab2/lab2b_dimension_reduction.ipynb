{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction\n",
    "\n",
    "In this notebook you will learn how to perform dimension reduction by using a few common techniques, namely data summaries, data conversion, and PCA. You will also use some common data exploration methods and learn how to make useful interactive plots. Most of the material is based on the examples from the text book.\n",
    "\n",
    "> (c) 2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck \n",
    ">\n",
    "> Code included in\n",
    ">\n",
    "> _Data Mining for Business Analytics: Concepts, Techniques, and Applications in Python_ (First Edition) \n",
    "> Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n",
    "\n",
    "Let's get started by importing all required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:48.530938Z",
     "iopub.status.busy": "2021-05-27T20:01:48.530304Z",
     "iopub.status.idle": "2021-05-27T20:01:49.391804Z",
     "shell.execute_reply": "2021-05-27T20:01:49.391255Z"
    }
   },
   "outputs": [],
   "source": [
    "import dmba\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We are going to use the Boston Housing dataset again. Let's load it directly from the text book Python's library and prepare it for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:49.395620Z",
     "iopub.status.busy": "2021-05-27T20:01:49.395047Z",
     "iopub.status.idle": "2021-05-27T20:01:49.421027Z",
     "shell.execute_reply": "2021-05-27T20:01:49.420526Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = dmba.load_data('BostonHousing.csv')\n",
    "df = df.rename(columns={'CAT. MEDV': 'CAT_MEDV'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data summaries\n",
    "\n",
    "Let's see how to explore common summary statistics for a particular variable, in this case CRIM (per capita crime rate by town). Some of the operations were already used in the `pandas` notebook. Now we will compute mean, standard deviation, min, max, median, length, and missing values of CRIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:49.479934Z",
     "iopub.status.busy": "2021-05-27T20:01:49.479443Z",
     "iopub.status.idle": "2021-05-27T20:01:49.484857Z",
     "shell.execute_reply": "2021-05-27T20:01:49.484349Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Mean : ', df.CRIM.mean())\n",
    "print('Std. dev : ', df.CRIM.std())\n",
    "print('Min : ', df.CRIM.min())\n",
    "print('Max : ', df.CRIM.max())\n",
    "print('Median : ', df.CRIM.median())\n",
    "print('Length : ', len(df.CRIM))\n",
    "print('Number of missing values : ', df.CRIM.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data summaries give us an idea of the distribution of CRIM, but as we discussed in the lectures, the best is to always have look at the histogram of numerical variables to fully understand them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x=\"CRIM\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that CRIM has a log normal distribution with most values concentrated between 0-1. We used `plotly` to draw the histogram. This library produces interactive plots. You can zoom and browse through the plot to explore it.\n",
    "\n",
    "Now, let's have a look at the box plot for CRIM using a log scale to better visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df, x=\"CRIM\", log_x=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make a box plot for the distribution of CRIM per CAT_MEDV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how to explore a particular variable. But we can also have a look at the summary statistics for all the variables in the dataset. So let's compute mean, standard dev., min, max, median, length, and missing values for all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:49.492088Z",
     "iopub.status.busy": "2021-05-27T20:01:49.491193Z",
     "iopub.status.idle": "2021-05-27T20:01:49.507831Z",
     "shell.execute_reply": "2021-05-27T20:01:49.507425Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'mean': df.mean(),\n",
    "              'sd': df.std(),\n",
    "              'min': df.min(),\n",
    "              'max': df.max(),\n",
    "              'median': df.median(),\n",
    "              'length': len(df),\n",
    "              'miss.val': df.isnull().sum(),\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data summaries are very useful for understanding the dataset. In this particular case we have a complete dataset, but we could have discovered that a particular variable has many missing records and decided to exclude it from the analysis, which is a form of dimension reduction.\n",
    "\n",
    "Another useful method is to look at the pair-wise correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:49.511585Z",
     "iopub.status.busy": "2021-05-27T20:01:49.511138Z",
     "iopub.status.idle": "2021-05-27T20:01:49.536195Z",
     "shell.execute_reply": "2021-05-27T20:01:49.535797Z"
    }
   },
   "outputs": [],
   "source": [
    "df.corr().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always useful to see all the numbers in table format, but even better to look at a figure, which shows insights more quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(15, 10)})\n",
    "dataplot = sns.heatmap(df.corr(), cmap=\"RdBu\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Do any correlations stand out? Do you think you can remove any variable based on the correlation matrix? If yes, does it make sense to remove it/them? \n",
    "\n",
    "We can also create pivot tables to explore the data. Let's see how to do it by checking the average values for MEDV grouped by RM and CHAS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:49.599495Z",
     "iopub.status.busy": "2021-05-27T20:01:49.599035Z",
     "iopub.status.idle": "2021-05-27T20:01:49.603276Z",
     "shell.execute_reply": "2021-05-27T20:01:49.602878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Binning RM variable\n",
    "df['RM_bin'] = pd.cut(df.RM, range(0, 10), labels=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table: checking average value of MEDV for all combinations of RM and CHAS\n",
    "pd.pivot_table(df, values='MEDV', index=['RM_bin'], columns=['CHAS'],\n",
    "               aggfunc=np.mean, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows us that low values of RM (average number of rooms) are only observed for CHAS = 0, which means properties not bounding the river, which is a good insight about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: bin the AGE variable with a bin size of 10 \n",
    "# then create a pivot table of AGE_bin vs CHAS that displays the median of MEDV \n",
    "# and do show the labels this time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion\n",
    "\n",
    "We can also use data conversion to reduce dimensions. Let's now have a look at aggregations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:49.736142Z",
     "iopub.status.busy": "2021-05-27T20:01:49.717854Z",
     "iopub.status.idle": "2021-05-27T20:01:49.739585Z",
     "shell.execute_reply": "2021-05-27T20:01:49.740023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting cross tabulation of two variables and converting it into percentages\n",
    "tbl = pd.crosstab(df.CAT_MEDV, df.ZN.astype('str'))\n",
    "tbl = tbl[['0.0'] + list(tbl.columns[2:]) + ['100.0']] # re-ordering columns\n",
    "propTbl = tbl / tbl.sum()\n",
    "propTbl.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:49.753925Z",
     "iopub.status.busy": "2021-05-27T20:01:49.753416Z",
     "iopub.status.idle": "2021-05-27T20:01:49.757160Z",
     "shell.execute_reply": "2021-05-27T20:01:49.756772Z"
    }
   },
   "outputs": [],
   "source": [
    "# And if we want to see the total counts\n",
    "tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(propTbl.transpose()*100., title=\"Distribution of CAT.MEDV by ZN\", \n",
    "             labels={'value':'Percentage'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you use this insight to reduce dimensions? Perhaps groupping similar variables together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make a similar plot that displays the distribution of CAT_MEDV per RAD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Principal Component Analysis (PCA)\n",
    "\n",
    "Let's now see how to compute principal components on two dimensions using a new dataset about cereals and their health rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:50.207169Z",
     "iopub.status.busy": "2021-05-27T20:01:50.206500Z",
     "iopub.status.idle": "2021-05-27T20:01:50.217254Z",
     "shell.execute_reply": "2021-05-27T20:01:50.216732Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "df_cereals = dmba.load_data('Cereals.csv')\n",
    "df_cereals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose the variables calories and rating to see how PCA works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting PCA\n",
    "pcs = PCA(n_components=2)\n",
    "pcs.fit(df_cereals[['calories', 'rating']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of components can be assessed using the explained variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:50.227200Z",
     "iopub.status.busy": "2021-05-27T20:01:50.226732Z",
     "iopub.status.idle": "2021-05-27T20:01:50.229952Z",
     "shell.execute_reply": "2021-05-27T20:01:50.230346Z"
    }
   },
   "outputs": [],
   "source": [
    "pcsSummary = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n",
    "                           'Proportion of variance': pcs.explained_variance_ratio_,\n",
    "                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\n",
    "pcsSummary = pcsSummary.transpose()\n",
    "pcsSummary.columns = ['PC1', 'PC2']\n",
    "pcsSummary.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first principal component holds over 86% of the variance and therefore most of information in the sense of variability.\n",
    "\n",
    "The `components_` field of `pcs` gives the individual components. The columns in this matrix are the principal components `PC1`, `PC2`. The rows are variables in the order they are found in the input matrix, `calories` and `rating`. This gives the linear coefficients for variables transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:50.237714Z",
     "iopub.status.busy": "2021-05-27T20:01:50.237183Z",
     "iopub.status.idle": "2021-05-27T20:01:50.240866Z",
     "shell.execute_reply": "2021-05-27T20:01:50.240424Z"
    }
   },
   "outputs": [],
   "source": [
    "pcsComponents = pd.DataFrame(pcs.components_.transpose(), \n",
    "                             columns=['PC1', 'PC2'], \n",
    "                             index=['calories', 'rating'])\n",
    "pcsComponents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `transform` method to get the scores, i.e. projected variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:50.250394Z",
     "iopub.status.busy": "2021-05-27T20:01:50.249949Z",
     "iopub.status.idle": "2021-05-27T20:01:50.253718Z",
     "shell.execute_reply": "2021-05-27T20:01:50.253315Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(pcs.transform(df_cereals[['calories', 'rating']]), \n",
    "                      columns=['PC1', 'PC2'])\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform a principal component analysis of the whole table ignoring the first three non-numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:50.262935Z",
     "iopub.status.busy": "2021-05-27T20:01:50.262443Z",
     "iopub.status.idle": "2021-05-27T20:01:50.282491Z",
     "shell.execute_reply": "2021-05-27T20:01:50.282032Z"
    }
   },
   "outputs": [],
   "source": [
    "pcs = PCA()\n",
    "pcs.fit(df_cereals.iloc[:, 3:].dropna(axis=0))\n",
    "pcsSummary = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n",
    "                           'Proportion of variance': pcs.explained_variance_ratio_,\n",
    "                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\n",
    "pcsSummary = pcsSummary.transpose()\n",
    "pcsSummary.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary.columns) + 1)]\n",
    "pcsSummary.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first two principal components holds over 92% of the variance. Now let's see which variables give the highest weight for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:50.293414Z",
     "iopub.status.busy": "2021-05-27T20:01:50.292937Z",
     "iopub.status.idle": "2021-05-27T20:01:50.296501Z",
     "shell.execute_reply": "2021-05-27T20:01:50.296111Z"
    }
   },
   "outputs": [],
   "source": [
    "pcsComponents = pd.DataFrame(pcs.components_.transpose(), \n",
    "                             columns=pcsSummary.columns, \n",
    "                             index=df_cereals.iloc[:, 3:].columns)\n",
    "pcsComponents.iloc[:,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that sodium and potassium have the highest weights for PC1 and PC2, which would indicate them to be the most important variables to explain variance in the data. However, this is likely to be a scaling effect, just have a look at the original records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cereals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That highlights the importance of data standardization before running PCA, which we will now do by using the scale method of preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:50.322075Z",
     "iopub.status.busy": "2021-05-27T20:01:50.321552Z",
     "iopub.status.idle": "2021-05-27T20:01:50.325464Z",
     "shell.execute_reply": "2021-05-27T20:01:50.324964Z"
    }
   },
   "outputs": [],
   "source": [
    "pcs = PCA()\n",
    "pcs.fit(preprocessing.scale(df_cereals.iloc[:, 3:].dropna(axis=0)))\n",
    "pcsSummary = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n",
    "                           'Proportion of variance': pcs.explained_variance_ratio_,\n",
    "                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\n",
    "pcsSummary = pcsSummary.transpose()\n",
    "pcsSummary.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary.columns) + 1)]\n",
    "pcsSummary.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that we need 7 components to explain over 90% of the variance in the data. Let's see which variables have highest weigths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T20:01:50.336784Z",
     "iopub.status.busy": "2021-05-27T20:01:50.336288Z",
     "iopub.status.idle": "2021-05-27T20:01:50.339907Z",
     "shell.execute_reply": "2021-05-27T20:01:50.339450Z"
    }
   },
   "outputs": [],
   "source": [
    "pcsComponents = pd.DataFrame(pcs.components_.transpose(), \n",
    "                             columns=pcsSummary.columns, \n",
    "                             index=df_cereals.iloc[:, 3:].columns)\n",
    "pcsComponents.iloc[:,:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiber, potassium and rating have high weigths for PC1. Followed by calories, fat, sugars and weight for PC2. Let's now visualize the first two principal components and see whether it brings extra insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping records with NaN values\n",
    "df_cereals_red = df_cereals.dropna(axis=0)\n",
    "df_cereals_red = df_cereals_red.reset_index(drop=True)\n",
    "\n",
    "# Re-projecting data to new system\n",
    "scores = pd.DataFrame(pcs.fit_transform(preprocessing.scale(df_cereals_red.iloc[:, 3:])), \n",
    "                      columns=[f'PC{i}' for i in range(1, 14)])\n",
    "\n",
    "# Adding column with cereal names\n",
    "df_pca = pd.concat([df_cereals_red['name'], scores[['PC1', 'PC2']]], axis=1)\n",
    "\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_pca, x=\"PC1\", y=\"PC2\", text=\"name\", height=700)\n",
    "fig.update_traces(textposition=\"bottom center\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs",
   "language": "python",
   "name": "labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
