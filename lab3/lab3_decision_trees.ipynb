{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression Trees\n",
    "\n",
    "In this notebook you will learn how to build classification and regression trees for prediction. Furthermore, you will learn how to evaluate the performance of these models. Most of this notebook content is based on the examples from the text book.\n",
    "\n",
    "> (c) 2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck \n",
    ">\n",
    "> Code included in\n",
    ">\n",
    "> _Data Mining for Business Analytics: Concepts, Techniques, and Applications in Python_ (First Edition) \n",
    "> Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n",
    "\n",
    "Let's start by importing all the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T01:01:15.690135Z",
     "iopub.status.busy": "2021-02-25T01:01:15.689582Z",
     "iopub.status.idle": "2021-02-25T01:01:16.497732Z",
     "shell.execute_reply": "2021-02-25T01:01:16.497138Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import math\n",
    "import pydotplus\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error\n",
    "from sklearn import tree\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The very first thing we will do is to load the dataset. If you look at our folder structure, we have a folder called `data` inside the `lab3` folder. This is the place where we will store our datasets. Up to now we loaded data from the same folder as the notebook or from the `dmba` library. But you can tell Python the location of your dataset and load the data directly from there. Now we will see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your current working directory\n",
    "CWD = os.getcwd()\n",
    "CWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full path of the dataset\n",
    "DATA_DIR = 'data'\n",
    "filename = 'UniversalBank.csv'\n",
    "FILE_PATH = os.path.join(CWD, DATA_DIR, filename)\n",
    "FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can finally load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a quick look at this dataset. You can use the commands you learned in the privious lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how do we print the first lines?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what is the dimension of the data, i.e. number of rows and columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what are the data types?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what are the statistics for the numerical variables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset that doesn't need any cleaning. So we can go ahead and directly train a model. But bare in mind that in real life that almost never happens. For most real-life problems it takes quite some effort to prepare data before being able to use it to train models: the pre-processing steps of the data mining process.\n",
    "\n",
    "\n",
    "## Classification trees\n",
    "\n",
    "Now let's build our first classification model. We want to train a model to recognize when someone is likely to take a personal loan. The very first thing we need to do is to split our data into predictors and outcome. We know that our outcome is personal loan. Next, we need to decide which columns we want to use as predictors. We would now normally do some dimension reduction, also known as feature selection. But to keep things simple and concentrate on how to build a decision tree, we will simply remove ID and zip code and keep the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['ID', 'ZIP Code', 'Personal Loan']) # features/predictors\n",
    "y = df['Personal Loan'] # outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data partition\n",
    "\n",
    "The next step is to divide our data into training and validation sets. As you should remember from the lectures, this is important because we will use the train set to train the model and the validation set to evaluate model performance.\n",
    "\n",
    "This is very easy to do with the scikit-learn library, which provides a lot of functionalities for data mining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train/validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a few questions to see whether you understood what happened above:\n",
    "\n",
    "1. What is the name of the function used to split the data?\n",
    "2. What is test_size?\n",
    "3. What is random_state?\n",
    "4. Why do we have these 4 variables separated with commas?\n",
    "5. What would have happened if we had assigned the outome of this function to one variable?\n",
    "\n",
    "TIP: have a look at the docstring of the function as we learned last time (shift + tab).\n",
    "\n",
    "\n",
    "### Choosing technique\n",
    "\n",
    "We will use the DecisionTreeClassifier functionality of scikit-learn as our decision tree algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classTree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying algorithm & interpreting results\n",
    "\n",
    "And we will train a model with the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what do we need to give as input for fit?\n",
    "# Replace <input1> and <input2> for your answer\n",
    "classTree.fit(<input1>, <input2>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to visualize the results, but before, we will write a function for it, since we will be repeating tree visualizaion a few times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_visualization(decisionTree, feature_names=None, class_names=None):\n",
    "    \n",
    "    plt.figure(figsize=(60, 30))\n",
    "    tree.plot_tree(decisionTree, filled=True, rounded=True, impurity=False,\n",
    "                   feature_names=feature_names, class_names=class_names, label='root')\n",
    "\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call the function to visualize the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_visualization(classTree, feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to know the performance of our model. We can get the confusion matrix and accuracy by calling the following scikit-learn functions, but we will also be repeating this process a few times today. So can you make a function out of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance with training data\n",
    "y_pred = classTree.predict(X_train)\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "settype = 'Train'\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cbar=False)\n",
    "plt.title('%s accuracy = %f' % (settype, accuracy))\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a function to check model performance using the code from the cell above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: now get the performance for the validation data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a good model? We discussed that during the video lecture and the Q&A session.\n",
    "\n",
    "One way of testing whether a model is good is by running a sensitivity test, which we can do using k-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T01:01:17.710639Z",
     "iopub.status.busy": "2021-02-25T01:01:17.710181Z",
     "iopub.status.idle": "2021-02-25T01:01:17.749539Z",
     "shell.execute_reply": "2021-02-25T01:01:17.749955Z"
    }
   },
   "outputs": [],
   "source": [
    "# Five-fold cross-validation of the full decision tree classifier\n",
    "treeClassifier = DecisionTreeClassifier()\n",
    "scores = cross_val_score(treeClassifier, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's have a look at the accuracy of each fold and their average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy scores of each fold:', np.round(scores, 3))\n",
    "print('Accuracy average:', np.round(scores.mean(), 3))\n",
    "print('Accuracy standard deviation:', np.round(scores.std(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the accuracy stable? It seems to be, but they are all very high. The highest accuracy (0.992) is quite different from the lowest (0.972).\n",
    "\n",
    "In order to build a reliable model we should avoid overfitting, which is what is happening with the model above. One way to do so is by limiting tree growth. As we learned in the video lecture, a common way to stop tree growth is by giving threshold values for the tree depth, number of samples to keep splitting a node, and the required impurity decrease. Now let's see how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T01:01:17.756264Z",
     "iopub.status.busy": "2021-02-25T01:01:17.755716Z",
     "iopub.status.idle": "2021-02-25T01:01:17.889009Z",
     "shell.execute_reply": "2021-02-25T01:01:17.888332Z"
    }
   },
   "outputs": [],
   "source": [
    "smallClassTree = DecisionTreeClassifier(max_depth=30, min_samples_split=20, min_impurity_decrease=0.01)\n",
    "smallClassTree.fit(X_train, y_train)\n",
    "\n",
    "tree_visualization(smallClassTree, feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a tree that is much easier to read. We can explain this model to anyone. And what is the accuracy now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T01:01:17.895186Z",
     "iopub.status.busy": "2021-02-25T01:01:17.894672Z",
     "iopub.status.idle": "2021-02-25T01:01:17.908339Z",
     "shell.execute_reply": "2021-02-25T01:01:17.908705Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: use the function you wrote above to show the performance for both train and validation sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the accuracy estimated with the validation set is as good as the accuracies obtained with a fully grown tree and similar to the one from the train set. But in this case we have a much simpler model that is easy to explain and much more likely to perform well with new data, as it is not modelling all the small variantions in the train data (i.e. noise).\n",
    "\n",
    "Now we know how to build a good classifier, preventing overfitting by limitting the tree growth. Moreover, we should be able to explain such a model to anyone. The problem here is that we had to choose thresholds for the tree depth, number of samples in a node to keep splitting it, and the reduction in impurity. We had some numbers above, but they will not always be good for every problem. So how can we choose them in an unbiased way? Well... if you remember the lecture, we saw that we can perform a grid search. A good way of performing a grid search is by starting with an initial guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with an initial guess for parameters\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30, 40], \n",
    "    'min_samples_split': [20, 40, 60, 80, 100], \n",
    "    'min_impurity_decrease': [0.0009, 0.001, 0.005, 0.01], \n",
    "}\n",
    "gridSearch = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, n_jobs=-1)\n",
    "gridSearch.fit(X_train, y_train)\n",
    "print('Initial score: ', gridSearch.best_score_)\n",
    "print('Initial parameters: ', gridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And subsequentially using the results to refine our search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T01:01:17.916011Z",
     "iopub.status.busy": "2021-02-25T01:01:17.915470Z",
     "iopub.status.idle": "2021-02-25T01:01:22.706243Z",
     "shell.execute_reply": "2021-02-25T01:01:22.707034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adapt grid based on result from initial grid search\n",
    "param_grid = {\n",
    "    'max_depth': list(range(2, 16)), \n",
    "    'min_samples_split': list(range(10, 25)), \n",
    "    'min_impurity_decrease': [0.0009, 0.001, 0.0011], \n",
    "}\n",
    "gridSearch = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, n_jobs=-1)\n",
    "gridSearch.fit(X_train, y_train)\n",
    "print('Improved score: ', gridSearch.best_score_)\n",
    "print('Improved parameters: ', gridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pick the best result and see what happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T01:01:22.719906Z",
     "iopub.status.busy": "2021-02-25T01:01:22.719277Z",
     "iopub.status.idle": "2021-02-25T01:01:22.906598Z",
     "shell.execute_reply": "2021-02-25T01:01:22.907189Z"
    }
   },
   "outputs": [],
   "source": [
    "bestClassTree = gridSearch.best_estimator_\n",
    "tree_visualization(bestClassTree, feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what is the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T01:01:22.909664Z",
     "iopub.status.busy": "2021-02-25T01:01:22.909254Z",
     "iopub.status.idle": "2021-02-25T01:01:22.925826Z",
     "shell.execute_reply": "2021-02-25T01:01:22.926244Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: use the function you wrote above to show the performance for both train and validation sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running a grid search we managed to improve the accuracy while mantaining interpretability and removing bias from our choice for the hyperparameters.\n",
    "\n",
    "\n",
    "## Homework: regression trees\n",
    "\n",
    "Let's now move to regression trees. They work very similarly to classification trees, but let's see the differences.\n",
    "\n",
    "We start, as always, by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ToyotaCorolla.csv'\n",
    "FILE_PATH = os.path.join(CWD, DATA_DIR, filename)\n",
    "\n",
    "df = pd.read_csv(FILE_PATH).iloc[:1000,:]\n",
    "df = df.rename(columns={'Age_08_04': 'Age', 'Quarterly_Tax': 'Tax'})\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define predictors and outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: why are we choosing these? \n",
    "# You could try and have a look at the data to see whether you agree, \n",
    "# perhaps you even come up with a better model? ;-)\n",
    "predictors = ['Age', 'KM', 'Fuel_Type', 'HP', 'Met_Color', \n",
    "              'Automatic', 'CC', 'Doors', 'Tax', 'Weight']\n",
    "outcome = 'Price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have some categorical variables, meaning that we need to create dummy variables from them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df[predictors], drop_first=True)\n",
    "y = df[outcome]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is data partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start right away with a grid search to look for the best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First guess to find optimized tree\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20, 25], \n",
    "    'min_impurity_decrease': [0, 0.001, 0.005, 0.01], \n",
    "    'min_samples_split': [10, 20, 30, 40, 50], \n",
    "}\n",
    "gridSearch = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, n_jobs=-1)\n",
    "gridSearch.fit(X_train, y_train)\n",
    "print('Initial parameters: ', gridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T01:01:25.257715Z",
     "iopub.status.busy": "2021-02-25T01:01:25.257273Z",
     "iopub.status.idle": "2021-02-25T01:01:27.656571Z",
     "shell.execute_reply": "2021-02-25T01:01:27.657241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's now refine our search with the results\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6, 7], \n",
    "    'min_impurity_decrease': [0, 0.001, 0.002, 0.003, 0.005, 0.006, 0.007, 0.008], \n",
    "    'min_samples_split': [15, 16, 18, 20, 21, 22, 23, 24, 25], \n",
    "}\n",
    "gridSearch = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, n_jobs=-1)\n",
    "gridSearch.fit(X_train, y_train)\n",
    "print('Improved parameters: ', gridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we pick the best result as our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regTree = gridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look at performance measures for the regression tree model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_performance(decisionTree, X, y, settype):\n",
    "\n",
    "    # True and predicted values\n",
    "    y_true = y.values\n",
    "    y_pred = decisionTree.predict(X)\n",
    "    \n",
    "    # Compute error\n",
    "    y_res = y_true - y_pred\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = [\n",
    "        ('Mean Error (ME)', sum(y_res) / len(y_res)),\n",
    "        ('Root Mean Squared Error (RMSE)', math.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        ('Mean Absolute Error (MAE)', sum(abs(y_res)) / len(y_res)),\n",
    "    ]\n",
    "    if all(yt != 0 for yt in y_true):\n",
    "        metrics.extend([\n",
    "            ('Mean Percentage Error (MPE)', 100 * sum(y_res / y_true) / len(y_res)),\n",
    "            ('Mean Absolute Percentage Error (MAPE)', 100 * sum(abs(y_res / y_true) / len(y_res))),\n",
    "        ])\n",
    "        \n",
    "    # Print results\n",
    "    fmt1 = '{{:>{}}} : {{:.4f}}'.format(max(len(m[0]) for m in metrics))\n",
    "    print('\\n%s regression statistics\\n' % settype)\n",
    "    for metric, value in metrics:\n",
    "        print(fmt1.format(metric, value))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance measures\n",
    "regression_performance(regTree, X_train, y_train, 'Train')\n",
    "regression_performance(regTree, X_valid, y_valid, 'Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T01:01:27.676464Z",
     "iopub.status.busy": "2021-02-25T01:01:27.676005Z",
     "iopub.status.idle": "2021-02-25T01:01:29.161957Z",
     "shell.execute_reply": "2021-02-25T01:01:29.162468Z"
    }
   },
   "outputs": [],
   "source": [
    "tree_visualization(regTree, feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you also know how to build a regression tree! :)\n",
    "\n",
    "You can explore both datasets further and play a bit with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs",
   "language": "python",
   "name": "labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
